关于为什么pytorch在反向传播的时候需要手动将梯度清零  
**理由1**: 梯度累加——每次获取1个epoch的数据，计算1次梯度，梯度不清零，不断累加，累加一定次数之后，根据累加的梯度更新网络参数，然后清空梯度，进行下次循环。一定情况下，batchsize越大，训练效果越好，梯度累加实现了batchsize的变相扩大，如果accumulation_step=8,则batchsize变相扩大8倍，是乞丐实验室解决显存受限的一个不错的trick，需注意，此时学习率也要适当放大。
```python
for i, (images, labels) in enumerate(train_loader):
    # 1. input output
    pred = model(images)
    loss = criterion(pred, labels)  # 计算损失

    # 2.backward
    optimizer.zero_grad()
    loss.backward()                 # 反向传播，计算当前梯度
    optimizer.step()                # 根据梯度，更新网络参数
# 一个batch数据，计算一次梯度，更新一次网络
```
梯度累加
```python
for i, (images, labels) in enumerate(train_loader):
    # 1. input output
    pred = model(images)
    loss = criterion(pred, labels) # 计算损失

    # 2.1 loss regularization
    loss = loss/accumulation_steps
    # 2.2 反向传播, 计算梯度
    loss.backward()

    # 3. 根据梯度，更新网络参数
    if (i+1) % accumulation_steps == 0:
        optimizer.step()          # 根据梯度，更新网络参数
        optimizer.zero_grad()     # 梯度清零


```
**理由2：**多任务学习  
标准的多任务学习流程 
```python
for i, data in enmuerate(train_loader):
    xs, ys = data
    pred1 = model1(xs)          # 前向计算，会产生用于梯度回传的计算图，图中存储了反向传播需要的中间结果
    pred2 = model2(xs)

    loss1 = loss_fn1(pred1, ys)
    loss2 = loss_fn2(lpred2, ys)

    *******
    loss = loss1 + loss2      # 到此，内存中生成了两张计算图，如果loss的来源组成更加复杂，内存的消耗会更大
    optimizer.zero_grad()
    loss.backward()           # 反向传播，计算梯度：此时调用了.backward()，会从内存中将计算图释放
    ++++++
    optimizer.step()          # 根据梯度，计算网络参数

```
借助累加梯度可以减少多任务学习中，计算梯度时，内存的消耗
```python
for i, data in enumerate(train_loader):
    xs, ys = data
    optimizer.zero_grad()

    pred1 = model1(xs)       # 生成计算图1
    loss1 = loss_fn1(pred1, ys)
    loss1.backward()         # 反向传播，计算梯度1，释放计算图1

    pred2 = model2(xs)       # 生成计算图2
    loss2 = loss_fn2(pred2, ys)
    loss2.backward()         # 反向传播，计算梯度2，完成梯度的累加，并释放计算图2

    optimizer.step()         # 根据累加梯度，计算网络参数

```
可见：利用累加梯度，可以在最多保存一张计算图的情况下进行多任务学习的训练，所以梯度累加的思路是对内存的极大友好