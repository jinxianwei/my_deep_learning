从一个比较好的baseline开始，然后每次调其中的一个超参数，重新训练模型，查看在验证集上的精度或是损失    
然后会得到一种直觉——有的超参数会比较重要，有的超参数可能并不会影响模型的效果——得到这个模型对这些超参数的敏感程度

大家在优化算法里面常用的是Adam，但Adam算法不见得比SGD好，在SGD调参调的足够好的情况下，通常是比Adam的效果要好——最后收敛出来的模型的泛化能力，收敛的速度或是精度通常比Adam要好，但是Adam对模型的超参数如lr没有那么敏感，所以调起来比较方便  
尝试若干次之后重复调参，就能够得到，这些超参数在哪个范围里是比较好的
好的baseline的超参数在哪里，如果数据集发生了改变，但是效果已经不好了，这该如何是好？  

任何调过参数的实验，最好都管理起来——保存训练的日志和超参数给记录下来，这样可以进行比较和分享给他人或是重复之前的实验
简单的参数比较少的话：可以用txt记录loss和lr等信息（在SSD模型中的用法）；如果超参数比较多的话，可以使用tensorboard 或 weight & bias (https://wandb.ai/site)

重复别人的实验是很难的：
1）环境或GPU硬件，用的python库（会发现写python的人有些不注重可维护性，加了许多新的特征不断的发布版本，但是却有些不注重新版本和旧版本的兼容性，API，默认的超参数如padding的默认值，同样的接口同样的东西可能都会得到不同的结果）——原因在于：发展太快，每个人都往前赶，加一些新的特征；这也是python生态圈一直被大家诟病的事情  
2）代码的开发，如果版本控制做的不好，就最好将代码都放在同一个地方  
3）随机性，改变随机种子的话也会改变——此时的问题是代码模型的稳定性不够，一般是数据，因为在使用SGD的时候，数据的batch，又或是droupout等python库里面带来的一定的随机性——尽量避免当换一个随机种子导致结果的剧烈变化（实在不行的话，加权平均多个模型可能也是一个途径吧）  
总之，在deeplearning领域，完全重复一个模型是很难的，只能是差不多，因为随机性的存在

整个趋势：计算成本在下降，但人力的成本在增加
所以在小的任务上，已经可以用机器自动的调参
![在这里插入图片描述](https://img-blog.csdnimg.cn/96acc16f309b4d45aa4127b3281652f0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBASHNpZW5XZWktQ2hpbg==,size_20,color_FFFFFF,t_70,g_se,x_16)
模型是逐步增大的，如果对时间的要求高，就选比较小的网络，如果对精度要求高一点，就选比较大的网络
batch_size的大小，通常会取2的某些次方，在做计算的时候方便一点，因为线程数也是二的某些次方，这样更方便map到这些线程上面，否则，算完前面的，后面可能还多一点点，不能整除，导致计算性能会打一点折扣——但对优化收敛的结果来看，取什么样的数值都差不多  

最优超参数的获取方式：  
1）完全训练——每一组超参数，完全的训练结束，返回关键的衡量指标如loss等，进行对比（适合数据集小的，容易训练的网络）  
2）只考虑哪个超参数之间的优劣，得到一个近似值就行（适合数据集很大，模型很大的网络，训练一次时间太长）  
- 使用子数据集（对数据集进行采样：100万采样到10万）  
- 把模型变小（如：想调的是resnet152，但是真正训练的时候，可以用resnet18用来模拟），一个SGD上的超参数，如果对resnet18上的训练效果差不多的话，那么对resnet152可能也是不错的
- 训练的时候可能会有很多的epoch（如对数据训练100遍），如果一个超参数特别不好的话，可能训练一遍（1个epoch）就知道它不行，有的根本就不动，有的甚至往上走，所以可以不用完全训练完成
- 对比较靠谱的超参数，把这些超参数的模型训练完，对于不靠谱的，可以早点停掉，早期就可以把它淘汰掉

网格遍历搜索和只取n次的网格遍历搜索
![在这里插入图片描述](https://img-blog.csdnimg.cn/baec1482c75e479a8566b216cc9a56a6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBASHNpZW5XZWktQ2hpbg==,size_20,color_FFFFFF,t_70,g_se,x_16)  
在没有更好的方法之前，第一个试的就应该是随机搜索  
### **我觉得人年轻的时候，就应该多看一看不一样的事情，当年龄够大的时候，可能看遍了这个世界的时候，知道长什么样的时候，在某一个领域，再往里面深耕或深入，把某一个领域做的更好一点**


- 可行的策略：先挑16个超参数，训练25个epoch，记录这16个超参数对应的模型的精度，然后根据精度把一半的超参数丢弃掉；下一时刻，将剩下8个比较靠谱的超参数，训练50个epoch，记录这8个超参数的精度，根据精度将靠谱的4个超参数记录下来；然后对这4个超参数训练100个epoch  
每一次，把当前最好的超参数留下来，然后加训练一倍长的epoch，最后得到1~2个完整训练完的模型，得到最好的结果（靠谱的超参数，给它多一点的资源，不靠谱的给它少一点资源，每一次淘汰一半多训练一倍的时间，使得总时间开销——训练个数x训练时间是固定的）——**Successive Halving算法**  
所有的这些，都可以使用代码完成，不局限于手动调整  

Hyperband算法是更常用的算法

![在这里插入图片描述](https://img-blog.csdnimg.cn/06674a4f91f148ed8660325d292bffbc.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBASHNpZW5XZWktQ2hpbg==,size_20,color_FFFFFF,t_70,g_se,x_16)

**Successive Halving算法和Hyperband算法**的思想是：不要训练完，训练一些轮数的时候，把不靠谱的淘汰掉，从随机搜索开始，有用而且实现起来也比较简单（注意是用代码实现随机搜索n个超参数）

假设已经跑过很多特定任务的模型和数据集了，会发现：总有那么几个模型和那么几组超参数再各个地方都跑的比较好，所以很可能就试验这么几个就行了

可以验证：已有50个不同的数据集和特定的20组超参数，得到20组超参数中，再50个数据集上的结果都比较好的前5组超参数；当有一个新的数据集的时候，尝试这5组超参数就行了，从最好的开始尝试，可能尝试2~3次的结果就会很不错了，所以在很多时候，都不需要很fancy的搜索超参数的方法，只需要把最好的那几个记住就行了，**可以看竞赛中那些模型中的超参数**，其实也可以发现，刷到最高分的那些论文中的模型中的超参数，其实都相差不多。  
**所以最优的超参数这个数值是存在的，如果能找到它的话，超参数的搜索其实是很简单的事情**