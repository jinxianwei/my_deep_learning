1. 误差的计算  
![在这里插入图片描述](https://img-blog.csdnimg.cn/7338b411774e461e81677a2bd1d9ba2e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBASHNpZW5XZWktQ2hpbg==,size_20,color_FFFFFF,t_70,g_se,x_16)
![在这里插入图片描述](https://img-blog.csdnimg.cn/95bf999b3922454989e2784c56b7e19c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBASHNpZW5XZWktQ2hpbg==,size_20,color_FFFFFF,t_70,g_se,x_16)

![在这里插入图片描述](https://img-blog.csdnimg.cn/e675ed6f72174617aa40ca4c53888868.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBASHNpZW5XZWktQ2hpbg==,size_20,color_FFFFFF,t_70,g_se,x_16)  
使用softmax处理后，会满足输出概率和为1，而使用sigmoid，每个输出节点之间时互不相干的，输出不满足任何分布   
softmax更常用，类似猫和狗的类别，不属于多个类别的情况；sigmoid的情况更常用于人类和男人这样的类别的分类  

2. 误差的反向传播  
   ![在这里插入图片描述](https://img-blog.csdnimg.cn/2ca6ae58df2242488a4e9796e96481cc.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBASHNpZW5XZWktQ2hpbg==,size_20,color_FFFFFF,t_70,g_se,x_16)

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/17485f73ed1c4cb6b7290c788749719e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBASHNpZW5XZWktQ2hpbg==,size_20,color_FFFFFF,t_70,g_se,x_16)

   **权重的更新**
   ![在这里插入图片描述](https://img-blog.csdnimg.cn/b1f0561f642d4ea88aa894366d80d256.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBASHNpZW5XZWktQ2hpbg==,size_20,color_FFFFFF,t_70,g_se,x_16)  
   若使用全部样本集进行求解，损失梯度会指向全局最优方向，但在实际应用中，往往不可能一次性将所有的数据载入内存（算力不够），所以只能分批次（batch）训练—— 若使用分批次样本进行求解损失，则梯度指向的是当前批次最优方向  

2. 优化器optimizer  
   优化器的目的——使网络更快的收敛  
   - SGD  
  $$ w_{t+1} = w_t-\alpha \cdot g(w_t) $$  
  $$ \alpha:学习率，g(w_t)为t时刻对参数w_t的损失梯度$$
  缺点：1）易受样本噪声影响；2）可能陷入局部最优解
   - SGD+Momentum  
  $$v_t = \eta\cdot v_{t-1} + \alpha\cdot g(w_t)$$
  $$ w_{t+1} = w_t -v_t$$
  $$ \alpha为学习率，g(w_t)为t时刻对参数w_t的梯度损失，\eta(0.9)为动量系数$$
  有人认为：momentum能够抑制由于标注过程中的错误标注而出现的噪声；有人认为加入前一个梯度的信息，能更有利于逃离局部最优点
   - Adagrad（自适应学习率）  
  $$ S_t = S_{t-1} + g(w_t)\cdot g(w_t)$$
  $$ w_{t+1} = w_t - \alpha /\sqrt{S_t + \epsilon} \cdot g(w_t)$$
  $$ \alpha 为学习率，g(w_t)为t时刻对参数w_t的损失梯度，\epsilon(10^{-7})为防止分母为零的小数$$
  St为平方求和，反映在学习率的分母上，分母越来越大，就会使得学习率越来越小  
  问题：Adagrad的学习率下降的太快可能还没收敛就停止了训练
   - RMSProp（自适应学习率）  
  $$ S_t = \eta\cdot S_{t-1} + (1-\eta)\cdot g(w_t)\cdot g(w_t)$$
  $$ w_{t+1} = w_t - \alpha/\sqrt{S_t+\epsilon}$$
  $$ \alpha为学习率，g(w_t)为t时刻对参数w_t的损失梯度，\eta(0.9)控制衰减速度，\epsilon(10^{-7})为防止分母为零的小数$$
   - Adam（自适应学习率）  
  $$ m_t = \beta_1\cdot m_{t-1} + (1-\beta_1)\cdot g(w_t) 一阶动量$$
  $$ v_t = \beta_2\cdot v_{t-1} + (1-\beta_2)\cdot g(w_t)\cdot g(w_t) 二阶动量$$
  根据两个动量组合关系，更新参数  

  **在实际使用过程中，常用的是：SGD,SGD+Momentum,Adam**
  

